{
  "cells": [
    {
      "cell_type": "code",
      "id": "UsVnU5L8EAFzEYWx7NxrAi3r",
      "metadata": {
        "tags": [],
        "id": "UsVnU5L8EAFzEYWx7NxrAi3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc5fb11-f6ad-4e1d-aecc-ad8cfedafe1e"
      },
      "source": [
        "# Quick exploration of the files in the provided folder\n",
        "\n",
        "# Import the Google Cloud Storage client library\n",
        "from google.cloud import storage\n",
        "import os # Often useful, though not strictly needed for this specific script\n",
        "\n",
        "# --- Configuration ---\n",
        "# Extract the bucket name and the prefix from the gs:// URI\n",
        "# The bucket is everything between gs:// and the first slash\n",
        "# The prefix is everything after the first slash (including it, to denote the path)\n",
        "bucket_name = 'labs.roitraining.com'\n",
        "# The prefix is like a directory path within the bucket.\n",
        "# Adding the trailing slash '/' typically tells GCS to list objects *within* this path structure.\n",
        "prefix = 'alaska-dept-of-snow/'\n",
        "\n",
        "# --- Script ---\n",
        "print(f\"Attempting to list contents of gs://{bucket_name}/{prefix}...\")\n",
        "\n",
        "try:\n",
        "    # Instantiate a storage client.\n",
        "    # This automatically uses Application Default Credentials (ADC)\n",
        "    # when running in a GCP environment with appropriate permissions.\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # List blobs (objects) in the specified bucket with the given prefix.\n",
        "    # The list_blobs method is efficient for iterating through objects matching a prefix.\n",
        "    # It handles pagination automatically for large numbers of files.\n",
        "    blobs_iterator = storage_client.list_blobs(bucket_name, prefix=prefix)\n",
        "\n",
        "    print(\"\\nContents found:\")\n",
        "    found_content = False\n",
        "    # Iterate through the iterator and print each blob's name\n",
        "    for blob in blobs_iterator:\n",
        "        # The blob.name is the full path of the object within the bucket\n",
        "        print(f\"- {blob.name}\")\n",
        "        found_content = True\n",
        "\n",
        "    if not found_content:\n",
        "        print(\"No objects found at this location, or the location does not exist within the bucket.\")\n",
        "        print(\"(Note: GCS folders are virtual; this lists objects whose names start with the prefix.)\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {e}\")\n",
        "    print(\"\\nPlease check:\")\n",
        "    print(\"1. That your GCP environment (e.g., the service account for your notebook) has the 'storage.objects.list' permission on the bucket.\")\n",
        "    print(\"2. That the bucket name ('labs.roitraining.com') and prefix ('alaska-dept-of-snow/') are correct.\")\n",
        "    print(\"3. Your internet connection.\")\n",
        "\n",
        "\n",
        "print(\"\\nListing attempt complete.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to list contents of gs://labs.roitraining.com/alaska-dept-of-snow/...\n",
            "\n",
            "Contents found:\n",
            "- alaska-dept-of-snow/.DS_Store\n",
            "- alaska-dept-of-snow/alaska-dept-of-snow-faqs.csv\n",
            "- alaska-dept-of-snow/faq-01.txt\n",
            "- alaska-dept-of-snow/faq-02.txt\n",
            "- alaska-dept-of-snow/faq-03.txt\n",
            "- alaska-dept-of-snow/faq-04.txt\n",
            "- alaska-dept-of-snow/faq-05.txt\n",
            "- alaska-dept-of-snow/faq-06.txt\n",
            "- alaska-dept-of-snow/faq-07.txt\n",
            "- alaska-dept-of-snow/faq-08.txt\n",
            "- alaska-dept-of-snow/faq-09.txt\n",
            "- alaska-dept-of-snow/faq-10.txt\n",
            "- alaska-dept-of-snow/faq-11.txt\n",
            "- alaska-dept-of-snow/faq-12.txt\n",
            "- alaska-dept-of-snow/faq-13.txt\n",
            "- alaska-dept-of-snow/faq-14.txt\n",
            "- alaska-dept-of-snow/faq-15.txt\n",
            "- alaska-dept-of-snow/faq-16.txt\n",
            "- alaska-dept-of-snow/faq-17.txt\n",
            "- alaska-dept-of-snow/faq-18.txt\n",
            "- alaska-dept-of-snow/faq-19.txt\n",
            "- alaska-dept-of-snow/faq-20.txt\n",
            "- alaska-dept-of-snow/faq-21.txt\n",
            "- alaska-dept-of-snow/faq-22.txt\n",
            "- alaska-dept-of-snow/faq-23.txt\n",
            "- alaska-dept-of-snow/faq-24.txt\n",
            "- alaska-dept-of-snow/faq-25.txt\n",
            "- alaska-dept-of-snow/faq-26.txt\n",
            "- alaska-dept-of-snow/faq-27.txt\n",
            "- alaska-dept-of-snow/faq-28.txt\n",
            "- alaska-dept-of-snow/faq-29.txt\n",
            "- alaska-dept-of-snow/faq-30.txt\n",
            "- alaska-dept-of-snow/faq-31.txt\n",
            "- alaska-dept-of-snow/faq-32.txt\n",
            "- alaska-dept-of-snow/faq-33.txt\n",
            "- alaska-dept-of-snow/faq-34.txt\n",
            "- alaska-dept-of-snow/faq-35.txt\n",
            "- alaska-dept-of-snow/faq-36.txt\n",
            "- alaska-dept-of-snow/faq-37.txt\n",
            "- alaska-dept-of-snow/faq-38.txt\n",
            "- alaska-dept-of-snow/faq-39.txt\n",
            "- alaska-dept-of-snow/faq-40.txt\n",
            "- alaska-dept-of-snow/faq-41.txt\n",
            "- alaska-dept-of-snow/faq-42.txt\n",
            "- alaska-dept-of-snow/faq-43.txt\n",
            "- alaska-dept-of-snow/faq-44.txt\n",
            "- alaska-dept-of-snow/faq-45.txt\n",
            "- alaska-dept-of-snow/faq-46.txt\n",
            "- alaska-dept-of-snow/faq-47.txt\n",
            "- alaska-dept-of-snow/faq-48.txt\n",
            "- alaska-dept-of-snow/faq-49.txt\n",
            "- alaska-dept-of-snow/faq-50.txt\n",
            "\n",
            "Listing attempt complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick exploration of the content in the first several files\n",
        "\n",
        "# Import the Google Cloud Storage client library\n",
        "from google.cloud import storage\n",
        "from google.api_core import exceptions # Import for specific error types\n",
        "\n",
        "# --- Configuration ---\n",
        "bucket_name = 'labs.roitraining.com'\n",
        "\n",
        "# List of the specific object names you want to preview\n",
        "# Taken directly from your output list (the first 5)\n",
        "object_names_to_preview = [\n",
        "    'alaska-dept-of-snow/.DS_Store',\n",
        "    'alaska-dept-of-snow/alaska-dept-of-snow-faqs.csv',\n",
        "    'alaska-dept-of-snow/faq-01.txt',\n",
        "    'alaska-dept-of-snow/faq-02.txt',\n",
        "    'alaska-dept-of-snow/faq-03.txt',\n",
        "]\n",
        "\n",
        "preview_limit = 1000 # Limit the number of characters to print for each preview\n",
        "\n",
        "# --- Script ---\n",
        "print(f\"Attempting to preview the content of the first {len(object_names_to_preview)} files...\")\n",
        "\n",
        "try:\n",
        "    # Instantiate a storage client (uses ADC)\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Loop through the specified object names and print content\n",
        "    for object_name in object_names_to_preview:\n",
        "        print(\"\\n\" + \"=\" * 60) # Separator for clarity\n",
        "        print(f\"Previewing: gs://{bucket_name}/{object_name}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Get a reference to the blob (object) directly without calling get_bucket() first\n",
        "        # This approach doesn't require storage.buckets.get permission\n",
        "        blob = storage_client.bucket(bucket_name).blob(object_name)\n",
        "\n",
        "        try:\n",
        "            # Check if the blob exists before trying to download\n",
        "            # This check *also* requires permissions, often storage.objects.get or storage.objects.list\n",
        "            # If this fails, the download will likely also fail, but this check is cleaner.\n",
        "            # In some strict permission scenarios, checking existence might require a different permission than reading.\n",
        "            # Let's proceed directly to download and handle potential errors there as well.\n",
        "            # if not blob.exists():\n",
        "            #     print(\"  [Info] File not found or you lack permission to check its existence.\")\n",
        "            #     continue\n",
        "\n",
        "            # Download the blob's content as a string.\n",
        "            # This explicitly requires storage.objects.get permission on *this specific object*.\n",
        "            content = blob.download_as_string().decode('utf-8')\n",
        "\n",
        "            # Print limited preview\n",
        "            if len(content) > preview_limit:\n",
        "                print(\"--- Content (first {} characters) ---\".format(preview_limit))\n",
        "                print(content[:preview_limit] + \"\\n...\") # Add ellipsis to show it's truncated\n",
        "                print(\"------------------------------------\")\n",
        "            else:\n",
        "                print(\"--- Content ---\")\n",
        "                print(content)\n",
        "                print(\"---------------\")\n",
        "\n",
        "        except exceptions.NotFound:\n",
        "             print(\"  [Error] File not found or you lack permission to read it.\")\n",
        "        except UnicodeDecodeError:\n",
        "            print(\"  [Error] Could not decode file content as UTF-8. It might be a binary file (.DS_Store is likely binary).\")\n",
        "        except exceptions.PermissionDenied:\n",
        "             print(f\"  [Error] Permission denied to read object: {object_name}. You need 'storage.objects.get'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"  [Error] An unexpected error occurred while reading {object_name}: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    # This outer catch block would only trigger if storage.Client() itself fails\n",
        "    # or if something goes wrong *before* the loop starts, which is less likely now.\n",
        "    print(f\"\\nAn error occurred during client setup: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\nPreview attempt complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2oHs-nLWexK",
        "outputId": "c41c59a8-9e13-4d5f-c17f-61e91c5f3820"
      },
      "id": "P2oHs-nLWexK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to preview the content of the first 5 files...\n",
            "\n",
            "============================================================\n",
            "Previewing: gs://labs.roitraining.com/alaska-dept-of-snow/.DS_Store\n",
            "============================================================\n",
            "  [Error] Could not decode file content as UTF-8. It might be a binary file (.DS_Store is likely binary).\n",
            "\n",
            "============================================================\n",
            "Previewing: gs://labs.roitraining.com/alaska-dept-of-snow/alaska-dept-of-snow-faqs.csv\n",
            "============================================================\n",
            "--- Content (first 1000 characters) ---\n",
            "\"question\",\"answer\"\n",
            "\"When was the Alaska Department of Snow established?\",\"The Alaska Department of Snow (ADS) was established in 1959, coinciding with Alaska’s admission as a U.S. state.\"\n",
            "\"What is the mission of the Alaska Department of Snow?\",\"Our mission is to ensure safe, efficient travel and infrastructure continuity by coordinating snow removal services across the state’s 650,000 square miles.\"\n",
            "\"How does ADS coordinate plowing across different regions?\",\"ADS works with local municipalities and regional offices to schedule and prioritize plowing routes, focusing first on high-traffic roads, emergency routes, and schools.\"\n",
            "\"Who do I contact to report an unplowed road?\",\"Contact your local ADS regional office. Each region maintains a hotline for snow-related service requests and emergencies.\"\n",
            "\"Does ADS oversee school closure decisions?\",\"While ADS provides data on snow conditions, final school closure decisions are made by local school districts. ADS coordinates closely to keep them\n",
            "...\n",
            "------------------------------------\n",
            "\n",
            "============================================================\n",
            "Previewing: gs://labs.roitraining.com/alaska-dept-of-snow/faq-01.txt\n",
            "============================================================\n",
            "--- Content ---\n",
            "When was the Alaska Department of Snow established?\n",
            "\n",
            "The Alaska Department of Snow (ADS) was established in 1959, coinciding with Alaska’s admission as a U.S. state.\n",
            "\n",
            "---------------\n",
            "\n",
            "============================================================\n",
            "Previewing: gs://labs.roitraining.com/alaska-dept-of-snow/faq-02.txt\n",
            "============================================================\n",
            "--- Content ---\n",
            "What is the mission of the Alaska Department of Snow?\n",
            "\n",
            "Our mission is to ensure safe, efficient travel and infrastructure continuity by coordinating snow removal services across the state’s 650,000 square miles.\n",
            "\n",
            "---------------\n",
            "\n",
            "============================================================\n",
            "Previewing: gs://labs.roitraining.com/alaska-dept-of-snow/faq-03.txt\n",
            "============================================================\n",
            "--- Content ---\n",
            "How does ADS coordinate plowing across different regions?\n",
            "\n",
            "ADS works with local municipalities and regional offices to schedule and prioritize plowing routes, focusing first on high-traffic roads, emergency routes, and schools.\n",
            "\n",
            "---------------\n",
            "\n",
            "Preview attempt complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data to BQ from the CSV, after validating the CSV contains the same info as the txt files\n",
        "\n",
        "# Script to load FAQ CSV data to BigQuery for use in RAG runtime (using genai_app dataset)\n",
        "\n",
        "from google.cloud import bigquery\n",
        "import os\n",
        "import re\n",
        "\n",
        "# --- Configuration ---\n",
        "# GCS URI for the source CSV file\n",
        "gcs_uri = \"gs://labs.roitraining.com/alaska-dept-of-snow/alaska-dept-of-snow-faqs.csv\"\n",
        "\n",
        "# *** Updated Dataset ID ***\n",
        "dataset_id = \"genai_app\"\n",
        "\n",
        "# Generate table ID from the filename part of the GCS URI\n",
        "# Extract the filename (e.g., 'alaska-dept-of-snow-faqs.csv')\n",
        "filename = gcs_uri.split('/')[-1]\n",
        "\n",
        "# Remove the .csv extension (e.g., 'alaska-dept-of-snow-faqs')\n",
        "table_name_base = filename.rsplit('.', 1)[0]\n",
        "\n",
        "# Replace hyphens with underscores and convert to lowercase for BQ table ID\n",
        "table_id = table_name_base.replace('-', '_').lower() # -> 'alaska_dept_of_snow_faqs'\n",
        "\n",
        "# Use your Qwiklabs project ID\n",
        "# Ensure this is the correct project where you want the dataset and table to exist\n",
        "project_id = \"qwiklabs-gcp-03-28c3125acb2b\" # <<< VERIFY THIS IS YOUR CURRENT PROJECT ID\n",
        "\n",
        "# --- Initialize the BigQuery client ---\n",
        "# Initialize the client with your project ID.\n",
        "# This uses Application Default Credentials (ADC) from your environment.\n",
        "try:\n",
        "    client = bigquery.Client(project=project_id)\n",
        "    print(f\"BigQuery client initialized for project: {client.project}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing BigQuery client: {e}\")\n",
        "    print(\"Please ensure you are running in a GCP environment with appropriate credentials.\")\n",
        "    exit() # Exit if client cannot be initialized\n",
        "\n",
        "# Construct the full table ID string (format: project.dataset.table)\n",
        "table_id_full = f\"{client.project}.{dataset_id}.{table_id}\"\n",
        "\n",
        "# --- Load Job Configuration ---\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,  # Assuming your CSV has a header row to skip\n",
        "    # *** Explicitly define the schema ***\n",
        "    # This overrides autodetect and ensures columns are 'question' and 'answer'\n",
        "    schema=[\n",
        "        bigquery.SchemaField(\"question\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"answer\", \"STRING\"),\n",
        "    ],\n",
        "    # *** How to write data ***\n",
        "    # WRITE_TRUNCATE: If the table exists, overwrite the data. If it doesn't, create it.\n",
        "    # WRITE_APPEND: If the table exists, append the data. If it doesn't, create it.\n",
        "    # WRITE_EMPTY: If the table exists and contains data, the job fails. If it's empty or doesn't exist, create it.\n",
        "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        ")\n",
        "\n",
        "# --- Start the Load Job ---\n",
        "print(f\"\\nStarting load job for '{gcs_uri}' into '{table_id_full}'...\")\n",
        "\n",
        "try:\n",
        "    load_job = client.load_table_from_uri(\n",
        "        gcs_uri,\n",
        "        table_id_full,\n",
        "        job_config=job_config,\n",
        "    )  # API request\n",
        "\n",
        "    print(f\"Load job {load_job.job_id} initiated. Waiting for job to complete...\")\n",
        "\n",
        "    # Wait for the job to complete. result() blocks until the job is done.\n",
        "    load_job.result()\n",
        "\n",
        "    print(\"Load job finished successfully.\")\n",
        "\n",
        "    # Verify the load by getting the destination table and checking row count\n",
        "    try:\n",
        "        destination_table = client.get_table(table_id_full)\n",
        "        print(f\"Successfully loaded {destination_table.num_rows} rows into {table_id_full}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Load job finished, but could not get table details after load: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during the load job: {e}\")\n",
        "    print(\"\\nPlease check:\")\n",
        "    print(f\"1. That the GCS URI '{gcs_uri}' is correct and accessible.\")\n",
        "    print(f\"2. That the BigQuery dataset '{dataset_id}' exists in project '{client.project}'.\")\n",
        "    print(\"3. That the service account running this notebook has the necessary permissions:\")\n",
        "    print(\"   - storage.objects.get on the GCS file.\")\n",
        "    print(f\"   - bigquery.tables.create and bigquery.tables.updateData on the '{dataset_id}' dataset in your project.\")\n",
        "    print(\"4. The format of the CSV file matches the expected two columns ('question', 'answer') and is valid CSV.\")\n",
        "\n",
        "\n",
        "print(\"\\nBigQuery load script complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksc0GEYDYmGE",
        "outputId": "77df98f9-d4d5-4cbc-c8ec-5a9fa48618b6"
      },
      "id": "ksc0GEYDYmGE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigQuery client initialized for project: qwiklabs-gcp-03-28c3125acb2b\n",
            "\n",
            "Starting load job for 'gs://labs.roitraining.com/alaska-dept-of-snow/alaska-dept-of-snow-faqs.csv' into 'qwiklabs-gcp-03-28c3125acb2b.genai_app.alaska_dept_of_snow_faqs'...\n",
            "Load job 1f893e8e-2485-416b-8db5-e37bdb6deb6e initiated. Waiting for job to complete...\n",
            "Load job finished successfully.\n",
            "Successfully loaded 50 rows into qwiklabs-gcp-03-28c3125acb2b.genai_app.alaska_dept_of_snow_faqs.\n",
            "\n",
            "BigQuery load script complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Script to create BigQuery ML model and embeddings table using Python (Update from previous method of running directly in BQ interface)\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.api_core import exceptions\n",
        "\n",
        "# --- Configuration ---\n",
        "# Your Qwiklabs project ID\n",
        "project_id = \"qwiklabs-gcp-03-28c3125acb2b\"\n",
        "\n",
        "# Your target dataset\n",
        "dataset_id = \"genai_app\"\n",
        "\n",
        "# Source table where your FAQ data is\n",
        "source_table_id = \"alaska_dept_of_snow_faqs\"\n",
        "\n",
        "# BigQuery ML model name\n",
        "model_id = \"embeddings_model\"\n",
        "\n",
        "# Name for the new table that will contain the embeddings\n",
        "# Convention: original_table_name_with_embeddings\n",
        "embeddings_table_id = f\"{source_table_id}_with_embeddings\" # -> 'alaska_dept_of_snow_faqs_with_embeddings'\n",
        "\n",
        "# Vertex AI connection name and location\n",
        "connection_id = \"us.vertex-rag-connection\"\n",
        "\n",
        "# Vertex AI text embedding model endpoint\n",
        "vertex_ai_endpoint = \"text-embedding-005\"\n",
        "\n",
        "# --- Initialize the BigQuery client ---\n",
        "try:\n",
        "    client = bigquery.Client(project=project_id)\n",
        "    print(f\"BigQuery client initialized for project: {client.project}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing BigQuery client: {e}\")\n",
        "    print(\"Please ensure you are running in a GCP environment with appropriate credentials.\")\n",
        "    exit()\n",
        "\n",
        "# Step 1: Create/Replace BigQuery ML Remote Model\n",
        "# Construct the full model ID string (format: project.dataset.model)\n",
        "model_id_full = f\"{client.project}.{dataset_id}.{model_id}\"\n",
        "\n",
        "# SQL query to create the remote model\n",
        "create_model_sql = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{model_id_full}`\n",
        "REMOTE WITH CONNECTION `{connection_id}`\n",
        "OPTIONS (ENDPOINT = '{vertex_ai_endpoint}');\n",
        "\"\"\"\n",
        "\n",
        "print(f\"\\nAttempting to create/replace BigQuery ML model: {model_id_full}\")\n",
        "print(f\"Using connection: {connection_id}\")\n",
        "print(f\"Pointing to Vertex AI endpoint: {vertex_ai_endpoint}\")\n",
        "\n",
        "try:\n",
        "    # Execute the SQL query as a BigQuery job\n",
        "    query_job_model = client.query(create_model_sql)\n",
        "\n",
        "    print(f\"Model creation job {query_job_model.job_id} initiated. Waiting for job to complete...\")\n",
        "\n",
        "    # Wait for the job to complete. result() blocks until the job is done.\n",
        "    query_job_model.result()\n",
        "\n",
        "    print(f\"BigQuery ML model '{model_id}' created/replaced successfully in dataset '{dataset_id}'.\")\n",
        "\n",
        "except exceptions.NotFound:\n",
        "    print(f\"\\nError: Connection '{connection_id}' not found.\")\n",
        "    print(\"Please ensure the connection exists and is in the correct location.\")\n",
        "except exceptions.PermissionDenied:\n",
        "    print(f\"\\nError: Permission denied to create model or use connection.\")\n",
        "    print(\"Ensure the service account has 'bigquery.models.create' and 'bigquery.connections.use' permissions.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during model creation: {e}\")\n",
        "    print(\"Please check the connection name, location, endpoint, and permissions.\")\n",
        "\n",
        "\n",
        "# --- Step 2: Create/Replace Embeddings Table ---\n",
        "# Construct the full source and destination table IDs\n",
        "source_table_id_full = f\"{client.project}.{dataset_id}.{source_table_id}\"\n",
        "embeddings_table_id_full = f\"{client.project}.{dataset_id}.{embeddings_table_id}\"\n",
        "\n",
        "# SQL query to create the embeddings table\n",
        "# It uses ML.GENERATE_EMBEDDING with the model created above\n",
        "# The inner query selects the data and concatenates question and answer into a 'content' column for embedding\n",
        "# SELECT * EXCEPT(content) selects all columns from the ML.GENERATE_EMBEDDING output *except* the intermediate 'content' column\n",
        "create_embeddings_table_sql = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{embeddings_table_id_full}` AS\n",
        "SELECT * EXCEPT(content)\n",
        "FROM ML.GENERATE_EMBEDDING(\n",
        "    MODEL `{model_id_full}`,\n",
        "    (SELECT question, answer, CONCAT(question, answer) AS content FROM `{source_table_id_full}`)\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "print(f\"\\nAttempting to create/replace embeddings table: {embeddings_table_id}\")\n",
        "print(f\"Using source data from: {source_table_id_full}\")\n",
        "print(f\"Using model: {model_id_full}\")\n",
        "\n",
        "try:\n",
        "    # Execute the SQL query as a BigQuery job\n",
        "    query_job_embedding = client.query(create_embeddings_table_sql)\n",
        "\n",
        "    print(f\"Embeddings table creation job {query_job_embedding.job_id} initiated. Waiting for job to complete...\")\n",
        "\n",
        "    # Wait for the job to complete.\n",
        "    # This job interacts with Vertex AI, so it might take longer than the model creation.\n",
        "    query_job_embedding.result()\n",
        "\n",
        "    print(f\"Embeddings table '{embeddings_table_id}' created/replaced successfully in dataset '{dataset_id}'.\")\n",
        "\n",
        "    # Optional: Verify row count in the new table\n",
        "    try:\n",
        "        destination_table = client.get_table(embeddings_table_id_full)\n",
        "        print(f\"Successfully created embeddings table with {destination_table.num_rows} rows.\")\n",
        "        # print(\"Schema of the new table:\")\n",
        "        # for field in destination_table.schema:\n",
        "        #     print(f\"- {field.name}: {field.field_type}\")\n",
        "    except Exception as e:\n",
        "         print(f\"Warning: Embeddings job finished, but could not get destination table details: {e}\")\n",
        "\n",
        "\n",
        "except exceptions.NotFound:\n",
        "     print(f\"\\nError: Source table '{source_table_id_full}' or Model '{model_id_full}' not found.\")\n",
        "     print(\"Ensure the source table exists and the model was created successfully.\")\n",
        "except exceptions.PermissionDenied:\n",
        "     print(f\"\\nError: Permission denied to create embeddings table or access data/model.\")\n",
        "     print(\"Ensure the service account has 'bigquery.tables.create', 'bigquery.tables.updateData', 'bigquery.tables.getData', and 'vertexai.endpoints.predict' permissions.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during embeddings table creation: {e}\")\n",
        "    print(\"Please check the source table, model, and permissions.\")\n",
        "\n",
        "\n",
        "print(\"\\nBigQuery ML and Embeddings table creation script complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3LKt0Oib-iy",
        "outputId": "f03224bc-8646-40ae-8693-0767e527846b"
      },
      "id": "a3LKt0Oib-iy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigQuery client initialized for project: qwiklabs-gcp-03-28c3125acb2b\n",
            "\n",
            "Attempting to create/replace BigQuery ML model: qwiklabs-gcp-03-28c3125acb2b.genai_app.embeddings_model\n",
            "Using connection: us.vertex-rag-connection\n",
            "Pointing to Vertex AI endpoint: text-embedding-005\n",
            "Model creation job 78dd3d8d-cb31-4458-806c-235e8686c86d initiated. Waiting for job to complete...\n",
            "BigQuery ML model 'embeddings_model' created/replaced successfully in dataset 'genai_app'.\n",
            "\n",
            "Attempting to create/replace embeddings table: alaska_dept_of_snow_faqs_with_embeddings\n",
            "Using source data from: qwiklabs-gcp-03-28c3125acb2b.genai_app.alaska_dept_of_snow_faqs\n",
            "Using model: qwiklabs-gcp-03-28c3125acb2b.genai_app.embeddings_model\n",
            "Embeddings table creation job 2a5d704e-069e-4113-b81e-b6fd80df2551 initiated. Waiting for job to complete...\n",
            "Embeddings table 'alaska_dept_of_snow_faqs_with_embeddings' created/replaced successfully in dataset 'genai_app'.\n",
            "Successfully created embeddings table with 50 rows.\n",
            "\n",
            "BigQuery ML and Embeddings table creation script complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Documenting Example Questions and expected types in this section\n",
        "\n",
        "validation_questions = {\n",
        "    \"q1\": {\"question\": \"What is the current snow forecast?\", \"type\": \"Current Weather\"},\n",
        "    \"q2\": {\"question\": \"Are there any school closures?\", \"type\": \"Closures\"},\n",
        "    \"q3\": {\"question\": \"What are the typical fees for snow removal? Our property value is $500,000\", \"type\": \"Fees\"},\n",
        "    \"q4\": {\"question\": \"How does the Alaska snow department prioritize what to plow first?\", \"type\": \"General\"},\n",
        "    \"q5\": {\"question\": \"Who makes the decisions on if a school will close?\", \"type\": \"Closures\"},\n",
        "}\n",
        "\n",
        "#print(validation_questions[\"q1\"][\"question\"])\n",
        "# Gemini Tool - Annual Fee Calculation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBBZaIaBe09I",
        "outputId": "2f3dca71-3955-422e-c9ba-27aada1f09b3"
      },
      "id": "HBBZaIaBe09I",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the current snow forecast?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Runtime Script\n",
        "\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import base64\n",
        "import json\n",
        "import requests\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from vertexai.evaluation import (\n",
        "    MetricPromptTemplateExamples,\n",
        "    EvalTask,\n",
        "    PairwiseMetric,\n",
        "    PairwiseMetricPromptTemplate,\n",
        "    PointwiseMetric,\n",
        "    PointwiseMetricPromptTemplate,\n",
        ")\n",
        "import datetime\n",
        "import pytest\n",
        "\n",
        "# Configuration\n",
        "PROJECT_ID = \"qwiklabs-gcp-03-28c3125acb2b\"\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "# For Weather API\n",
        "headers = {\n",
        "    'User-Agent': 'jason.lajeunesse@clearobject.com',\n",
        "    'Accept': 'application/ld+json'\n",
        "}\n",
        "\n",
        "lat, lon = 63.5888, -154.4931  # Alaska Lat/Long\n",
        "point_url = f'https://api.weather.gov/points/{lat},{lon}'\n",
        "\n",
        "# BigQuery details for RAG\n",
        "BQ_PROJECT_ID = PROJECT_ID # Usually the same as your main project ID\n",
        "BQ_DATASET_ID = \"genai_app\"\n",
        "BQ_SOURCE_TABLE_ID = \"alaska_dept_of_snow_faqs\"\n",
        "BQ_EMBEDDINGS_TABLE_ID = f\"{BQ_SOURCE_TABLE_ID}_with_embeddings\" # alaska_dept_of_snow_faqs_with_embeddings\n",
        "BQ_MODEL_ID = \"embeddings_model\" # The BQ ML model name\n",
        "\n",
        "# Tooling: Fee Calculation\n",
        "def calculate_snow_removal_fee(property_value: float) -> dict:\n",
        "    # Snow removal calculation based upon property value\n",
        "\n",
        "    # The fee is 0.1% of the property value per year\n",
        "    annual_fee_rate = 0.001\n",
        "    annual_fee = property_value * annual_fee_rate\n",
        "    # Return as a dictionary so the model can easily use the structured output\n",
        "    return {\"annual_fee\": round(annual_fee, 2)} # Round to 2 decimal places\n",
        "\n",
        "# Fee calculation tool for Gemini, which utilizes property value\n",
        "calculate_fee_tool = genai.types.Tool(\n",
        "    function_declarations=[\n",
        "        genai.types.FunctionDeclaration(\n",
        "            name='calculate_snow_removal_fee',\n",
        "            description='Calculates the annual snow removal fee based on the resident\\'s property value.',\n",
        "            parameters=genai.types.Schema(\n",
        "                type=\"OBJECT\",\n",
        "                properties={\n",
        "                    'property_value': genai.types.Schema(\n",
        "                        type=\"NUMBER\",\n",
        "                        description='The resident\\'s property value in US dollars.'\n",
        "                    )\n",
        "                },\n",
        "                required=['property_value']\n",
        "            )\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Map function names to actual Python functions\n",
        "available_functions = {\n",
        "    'calculate_snow_removal_fee': calculate_snow_removal_fee,\n",
        "}\n",
        "\n",
        "# Map function names to actual Python functions\n",
        "available_functions = {\n",
        "    'calculate_snow_removal_fee': calculate_snow_removal_fee,\n",
        "}\n",
        "\n",
        "# Gemini Client & Base Generation Function\n",
        "def get_gemini_client():\n",
        "    \"\"\"Initializes and returns the Gemini client.\"\"\"\n",
        "    return genai.Client(\n",
        "        vertexai=True,\n",
        "        project=PROJECT_ID,\n",
        "        location=LOCATION,\n",
        "    )\n",
        "\n",
        "def generate_response_tools(user_input, system_instruction=None, tools=None, contents=None):\n",
        "\n",
        "    # Updated generate response function to generate Gemini response, with streaming and tool outputs.\n",
        "    # Can take existing conversation history (`contents`) and tools.\n",
        "\n",
        "    client = get_gemini_client()\n",
        "    model_name = \"gemini-2.0-flash-001\" # Using flash for speed\n",
        "\n",
        "    # If contents is not provided, start a new conversation turn\n",
        "    if contents is None:\n",
        "         contents = [\n",
        "            types.Content(\n",
        "              role=\"user\",\n",
        "              parts=[types.Part.from_text(text=user_input)]\n",
        "            )\n",
        "         ]\n",
        "\n",
        "    generate_content_config = types.GenerateContentConfig(\n",
        "        temperature=0.3,\n",
        "        top_p=0.95,\n",
        "        max_output_tokens=8192,\n",
        "        response_modalities=[\"TEXT\"],\n",
        "        system_instruction=[types.Part.from_text(text=system_instruction)] if system_instruction else None,\n",
        "        tools=tools #now with tools\n",
        "    )\n",
        "\n",
        "    output_string = \"\"\n",
        "    function_call_part = None # To store a potential function call from the model\n",
        "\n",
        "    # The orchestration logic will handle the turns\n",
        "    response = client.models.generate_content(\n",
        "        model=model_name,\n",
        "        contents=contents,\n",
        "        config=generate_content_config,\n",
        "    )\n",
        "\n",
        "    # Process the response\n",
        "    # Check if the model wants to call a function\n",
        "    if response.candidates and response.candidates[0].content.parts:\n",
        "        for part in response.candidates[0].content.parts:\n",
        "            if part.function_call:\n",
        "                function_call_part = part\n",
        "                #print(f\"Gemini requested function call: {function_call_part.function_call.name}\")\n",
        "                return {\"function_call\": function_call_part}\n",
        "            if part.text:\n",
        "                # Accumulate text for the final response (after tool use, or if no tool needed)\n",
        "                output_string += part.text\n",
        "\n",
        "    #print(\"Gemini returned text.\")\n",
        "    return {\"text_response\": output_string}\n",
        "\n",
        "def generate_response(user_input, sys_instruction):\n",
        "\n",
        "  # Original simple text generation function still used for classification/safety\n",
        "\n",
        "  client = get_gemini_client()\n",
        "  model = \"gemini-2.0-flash-001\"\n",
        "\n",
        "  contents = [\n",
        "    types.Content(\n",
        "      role=\"user\",\n",
        "      parts=[\n",
        "        types.Part.from_text(text = user_input)\n",
        "      ]\n",
        "    )\n",
        "  ]\n",
        "\n",
        "  generate_content_config = types.GenerateContentConfig(\n",
        "    temperature = 0.1, # Lower temp for classification\n",
        "    top_p = 0.95,\n",
        "    max_output_tokens = 128, # Keep output short for classification\n",
        "    # response_modalities = [\"TEXT\"], # Text is default\n",
        "    system_instruction=[types.Part.from_text(text=sys_instruction)],\n",
        "  )\n",
        "\n",
        "  output_string = \"\"\n",
        "\n",
        "  # Using the original streaming approach for classification - it's fast and simple here\n",
        "  # print(\"Calling Gemini for classification...\")\n",
        "  try:\n",
        "      for chunk in client.models.generate_content_stream(\n",
        "        model = model,\n",
        "        contents = contents,\n",
        "        config = generate_content_config,\n",
        "        ):\n",
        "        output_string += chunk.text.replace('\\n', ' ').replace('\\r', '').strip() # Clean up output\n",
        "      return output_string\n",
        "  except Exception as e:\n",
        "      print(f\"Error during classification: {e}\")\n",
        "      return \"ErrorClassifying\" # Return a default error string\n",
        "\n",
        "def classify_question(user_question):\n",
        "\n",
        "  # Provides the classification of a question in one of the types of Employment, General Information, Emergency Services, or Tax Related\n",
        "\n",
        "  sys_instruction = \"\"\"\n",
        "    You are a query classification agent that will provide an answer back on the type of user question provided to you.\n",
        "    This is in support of another agent providing answers to residents of Alaska. But your focus is ONLY on classifying the type of query.\n",
        "\n",
        "    These are the four possible results you should return (and please return a single word only for the best fitting category):\n",
        "    Weather, Closures, Fees, or General (catch all bucket for Alaskan Snow Removal information that is not in the other 3 categories)\n",
        "  \"\"\"\n",
        "\n",
        "  classification = generate_response(user_question, sys_instruction).replace('\\n', ' ').replace('\\r', '').rstrip()\n",
        "\n",
        "  return classification\n",
        "\n",
        "def is_valid(user_question):\n",
        "\n",
        "  # Provides the classification of a question in one of the types of Employment, General Information, Emergency Services, or Tax Related\n",
        "\n",
        "  sys_instruction = \"\"\"\n",
        "    You are chatbot designed to check for valid user input.\n",
        "\n",
        "    User input is only valid if the question pertains to asking for information related to the Alaska Snow Department. Please note that\n",
        "    a user may not explicitly ask about the Alaska snow department, but questions should relate to topics like general information about\n",
        "    the Alaska Snow Department, current weather conditions, business or school closures, or fees related to the Alaskan Snow Department.\n",
        "    All other topics are considered Invalid.\n",
        "\n",
        "    Please respond with either Valid or Invalid\n",
        "  \"\"\"\n",
        "\n",
        "  validity = generate_response(user_question, sys_instruction).replace('\\n', ' ').replace('\\r', '').rstrip()\n",
        "\n",
        "  return validity\n",
        "\n",
        "def is_safe(content):\n",
        "\n",
        "  # Provides the classification of a question in one of the types of Employment, General Information, Emergency Services, or Tax Related\n",
        "\n",
        "  sys_instruction = \"\"\"\n",
        "    You are chatbot designed to check for valid content, which could be either from a model result or an input prompt to a model.\n",
        "\n",
        "    Any content provided that involves hate speach, physical or sexual harm, sexually explicit content, or dangerous material should be considered Unsafe.\n",
        "    Content that does not contain these dangerous elements can be considered Safe.\n",
        "\n",
        "    Please respond with whether the content is either Safe or Unsafe with a single word, Safe or Unsafe.\n",
        "  \"\"\"\n",
        "\n",
        "  safety = generate_response(content, sys_instruction).replace('\\n', ' ').replace('\\r', '').rstrip()\n",
        "\n",
        "  return safety\n",
        "\n",
        "def retrieve_augmentations(user_input_query):\n",
        "\n",
        "  # Retrieve the related Alaska Snow Removal FAQ infromation from BQ\n",
        "\n",
        "   # GCP Details\n",
        "  project_id = \"qwiklabs-gcp-03-28c3125acb2b\"\n",
        "  client = bigquery.Client(project=project_id)\n",
        "\n",
        "  # Initialize the BigQuery client\n",
        "  client = bigquery.Client()\n",
        "\n",
        "  # Step 1: Get the Top 5 Matching Results from BQ\n",
        "\n",
        "  # Setup query to get top 5 results from BQ via similarity search, with the Vertex AI model connection generating the embedding of the user input question\n",
        "  query = f\"\"\"\n",
        "  SELECT query.query, base.question, base.answer\n",
        "  FROM VECTOR_SEARCH(\n",
        "    TABLE `qwiklabs-gcp-03-28c3125acb2b.genai_app.alaska_dept_of_snow_faqs_with_embeddings`, 'ml_generate_embedding_result',\n",
        "    (\n",
        "    SELECT text_embedding, content AS query\n",
        "    FROM ML.GENERATE_TEXT_EMBEDDING(\n",
        "    MODEL `qwiklabs-gcp-03-28c3125acb2b.genai_app.embeddings_model`,\n",
        "    (SELECT @query_text AS content))\n",
        "    ),\n",
        "    top_k => 5, options => '{{\"fraction_lists_to_search\": 0.01}}')\n",
        "  \"\"\"\n",
        "\n",
        "  # Configuration of the user input parameter\n",
        "  job_config = bigquery.QueryJobConfig(\n",
        "    query_parameters = [\n",
        "        bigquery.ScalarQueryParameter(\"query_text\", \"STRING\", user_input_query),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  top5_results_df = client.query(query, job_config=job_config).to_dataframe()\n",
        "\n",
        "  return top5_results_df\n",
        "\n",
        "def get_weather_alaska_tonight():\n",
        "  # Step 1: Get gridpoint info\n",
        "  point_response = requests.get(point_url, headers=headers)\n",
        "  forecast_url = point_response.json()['forecast']\n",
        "\n",
        "  # Step 2: Get forecast from URL\n",
        "  forecast_response = requests.get(forecast_url, headers=headers)\n",
        "  forecast_data = forecast_response.json()\n",
        "\n",
        "  # Step 3: Print each forecast period\n",
        "  for period in forecast_data['periods']:\n",
        "      if period['name'] =='Tonight':\n",
        "        return period['detailedForecast']\n",
        "\n",
        "def chat_with_tools_and_rag(user_input_query):\n",
        "\n",
        "    # Handles the user query, classifying it and using either tools or RAG to generate a response.\n",
        "\n",
        "    # Step 1: Initial Checks (Validity & Safety)\n",
        "    if is_safe(user_input_query) == \"Unsafe\":\n",
        "        return \"Your query was flagged as potentially unsafe. Please rephrase your question.\"\n",
        "\n",
        "    # Check input validity (if it's relevant to the Snow Dept)\n",
        "    if is_valid(user_input_query) == \"Invalid\":\n",
        "         # Check output safety of this canned response just in case\n",
        "        if is_safe(\"Your query does not seem to be related to the Alaska Snow Department.\") == \"Unsafe\":\n",
        "             return \"An unexpected issue occurred.\"\n",
        "        return \"Your query does not seem to be related to the Alaska Snow Department. Please ask about snow removal in Alaska.\"\n",
        "\n",
        "    # Step 2: Classify the Query\n",
        "    query_type = classify_question(user_input_query)\n",
        "\n",
        "    # Step 3: Determine Response Strategy (Tools or RAG)\n",
        "    final_response = \"\"\n",
        "\n",
        "    if query_type == \"Fees\":\n",
        "        #print(\"Query classified as 'Fees'. Attempting to use the fee calculation tool.\")\n",
        "\n",
        "        # System instruction for the tool-using model\n",
        "        tool_system_instruction = \"\"\"\n",
        "        You are the helpful assistant for the Alaska Snow Department. The annual snow removal fee is calculated as 0.1% of the property value.\n",
        "        Use the available tool 'calculate_snow_removal_fee' to get the fee based on the user's request.\n",
        "        Once you have the result from the tool, state the annual fee clearly and politely to the user based on the property value they mentioned or that was used for calculation.\n",
        "        If the user's prompt does not contain a property value, ask the user for their property value so you can calculate the fee.\n",
        "        \"\"\"\n",
        "\n",
        "        # Conversation history starts with the user's turn\n",
        "        conversation_history = [types.Content(role=\"user\", parts=[types.Part.from_text(text=user_input_query)])]\n",
        "\n",
        "        # First turn: Ask the model if it needs a tool\n",
        "        response_turn1 = generate_response_tools(\n",
        "            user_input=None, # Pass user_input as part of contents history\n",
        "            system_instruction=tool_system_instruction,\n",
        "            tools=[calculate_fee_tool], # Provide the tool definition\n",
        "            contents=conversation_history # Pass the initial history\n",
        "        )\n",
        "\n",
        "        if \"function_call\" in response_turn1:\n",
        "            # Model wants to call a function\n",
        "            function_call_part = response_turn1[\"function_call\"]\n",
        "            function_name = function_call_part.function_call.name\n",
        "            function_args = function_call_part.function_call.args # The arguments\n",
        "\n",
        "            #print(f\"Model requested function: {function_name} with args: {function_args}\")\n",
        "\n",
        "            if function_name in available_functions:\n",
        "                # Call the actual Python function\n",
        "                try:\n",
        "                    # The args object is a protobuf message, convert to JSON then Python dict\n",
        "                    args_dict = {key: value for key, value in function_args.items()}\n",
        "                    tool_output_data = available_functions[function_name](**args_dict)\n",
        "\n",
        "                    #print(f\"Tool '{function_name}' executed. Output: {tool_output_data}\")\n",
        "\n",
        "                    # Prepare the function response for the model\n",
        "                    function_response_part = types.Part.from_function_response(\n",
        "                         name=function_name,\n",
        "                         response=tool_output_data # Pass the dictionary output\n",
        "                    )\n",
        "\n",
        "                    # Add the model's function call and the function response to the history\n",
        "                    conversation_history.append(function_call_part)\n",
        "                    conversation_history.append(types.Content(role=\"function\", parts=[function_response_part]))\n",
        "\n",
        "                    # Second turn: Send the tool output back to the model to get the final response\n",
        "                    response_turn2 = generate_response_tools(\n",
        "                        user_input=None, # Already in history\n",
        "                        system_instruction=tool_system_instruction, # Use the same instructions\n",
        "                        tools=[calculate_fee_tool], # Still provide the tool definition\n",
        "                        contents=conversation_history # Pass updated history\n",
        "                    )\n",
        "\n",
        "                    if \"text_response\" in response_turn2:\n",
        "                        final_response = response_turn2[\"text_response\"]\n",
        "                    else:\n",
        "                        final_response = \"Could not get a final text response after tool execution.\"\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error executing tool '{function_name}': {e}\")\n",
        "                    final_response = \"An error occurred while calculating the fee.\"\n",
        "\n",
        "            else:\n",
        "                # Model requested a function that doesn't exist\n",
        "                print(f\"Model requested unknown function: {function_name}\")\n",
        "                final_response = f\"The requested function '{function_name}' is not available.\"\n",
        "\n",
        "        elif \"text_response\" in response_turn1:\n",
        "            # Model didn't call a function, returned text directly (e.g., asking for property value)\n",
        "            final_response = response_turn1[\"text_response\"]\n",
        "            #print(\"Model returned text instead of calling tool (likely asking for input).\")\n",
        "\n",
        "        else:\n",
        "            final_response = \"An unexpected response was received from the model during the fees process.\"\n",
        "\n",
        "    elif query_type == \"Weather\":\n",
        "        #print(f\"Query classified as '{query_type}'. Performing weather lookup.\")\n",
        "\n",
        "        tonights_weather = get_weather_alaska_tonight()\n",
        "\n",
        "        weather_system_instruction = f\"\"\"\n",
        "          You are the helpful assistant for the Alaska Snow Department. Answer the user's question based on the weather information\n",
        "          provided (for tonight's forecast in Alaska). Please note to the user that only tonight's forecast is available, if asking\n",
        "          for a different timeframe.\n",
        "\n",
        "          Toinight's weather forecast: {tonights_weather}\n",
        "        \"\"\"\n",
        "        weather_response = generate_response_tools(\n",
        "            user_input=user_input_query,\n",
        "            system_instruction=weather_system_instruction,\n",
        "            tools=None # No tools needed for RAG response generation\n",
        "        )\n",
        "        if \"text_response\" in weather_response:\n",
        "             final_response = weather_response[\"text_response\"]\n",
        "        else:\n",
        "             weather_response = \"Could not generate a response based on the provided information.\"\n",
        "\n",
        "    else:\n",
        "        #print(f\"Query classified as '{query_type}'. Performing RAG lookup.\")\n",
        "\n",
        "        # Perform RAG lookup for other categories\n",
        "        augmentation_text = retrieve_augmentations(user_input_query)\n",
        "\n",
        "        # Create reference prompt which will be appended to the system instructions\n",
        "        prompt_qnas = \"Please Use the Reference Question And Answers to answer the user's query: \\n\"\n",
        "\n",
        "        for row in augmentation_text.itertuples():\n",
        "            prompt_qnas = prompt_qnas + f\"Question: {row.question}, Answer: {row.answer}\\n\"\n",
        "\n",
        "        # System instruction for the RAG model\n",
        "        rag_system_instruction = f\"\"\"\n",
        "          You are the helpful assistant for the Alaska Snow Department. Answer the user's question based ONLY on the information provided.\n",
        "          If the provided information does not contain the answer, state that you cannot find the answer in the provided information.\n",
        "          Do not use outside knowledge.\n",
        "\n",
        "          {prompt_qnas}\n",
        "        \"\"\"\n",
        "\n",
        "        # Generate response using RAG and the retrieved context\n",
        "        # Use generate_response_tools for more control, but pass user_input directly for non-tool cases\n",
        "        rag_response = generate_response_tools(\n",
        "            user_input=user_input_query,\n",
        "            system_instruction=rag_system_instruction,\n",
        "            tools=None # No tools needed for RAG response generation\n",
        "        )\n",
        "        if \"text_response\" in rag_response:\n",
        "             final_response = rag_response[\"text_response\"]\n",
        "        else:\n",
        "             final_response = \"Could not generate a response based on the provided information.\"\n",
        "\n",
        "\n",
        "    # Step 4: Final Safety Check of the generated response\n",
        "    if is_safe(final_response) == \"Unsafe\":\n",
        "        return \"The generated response was flagged as potentially unsafe.\" # Return generic message\n",
        "\n",
        "    return final_response\n",
        "\n",
        "def test_calculate_snow_removal_fee():\n",
        "\n",
        "    #Test for expected snow removal fee (0.1% of property value)\n",
        "\n",
        "    try:\n",
        "      property_value = 370000.0\n",
        "      fee = calculate_snow_removal_fee(property_value)\n",
        "      assert fee['annual_fee'] == 370.00\n",
        "\n",
        "      print(\"PASSED: test_calculcated_snow_removal_fee() tests\")\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"classify_question test FAILED: An error occurred - {e}\")\n",
        "\n",
        "def test_is_valid():\n",
        "\n",
        "    #Test for expected safety results\n",
        "    try:\n",
        "      #Test 1\n",
        "      user_input = \"Tell me about puppies\"\n",
        "      result = is_valid(user_input)\n",
        "      assert result == \"Invalid\"\n",
        "\n",
        "      #Test 2\n",
        "      user_input = \"How does the Alaskan Snow Department impact school closures?\"\n",
        "      result = is_valid(user_input)\n",
        "      assert result == \"Valid\"\n",
        "\n",
        "      print(\"PASSED: is_valid() tests\")\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"classify_question test FAILED: An error occurred - {e}\")\n",
        "\n",
        "def test_is_safe():\n",
        "\n",
        "    #Test for expected safety results\n",
        "    try:\n",
        "      #Test 1\n",
        "      user_input = \"Do something violent!\"\n",
        "      result = is_safe(user_input)\n",
        "      assert result == \"Unsafe\"\n",
        "\n",
        "      #Test 2\n",
        "      user_input = \"How does the Alaskan Snow Department impact school closures?\"\n",
        "      result = is_safe(user_input)\n",
        "      assert result == \"Safe\"\n",
        "\n",
        "      print(\"PASSED: is_safe() tests\")\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"classify_question test FAILED: An error occurred - {e}\")\n",
        "\n",
        "def test_classify_questions():\n",
        "\n",
        "    #Test for classifying - Weather, Closures, Fees, or General\n",
        "    try:\n",
        "      #Test 1\n",
        "      user_input = \"How is the weather looking?\"\n",
        "      result = classify_question(user_input)\n",
        "      assert result == \"Weather\"\n",
        "\n",
        "      #Test 2\n",
        "      user_input = \"How does the Alaskan Snow Department impact school closures?\"\n",
        "      result = classify_question(user_input)\n",
        "      assert result == \"Closures\"\n",
        "\n",
        "      #Test 3\n",
        "      user_input = \"What are the typical fees for snow removal in Alaska? Our property value is $370,000?\"\n",
        "      result = classify_question(user_input)\n",
        "      assert result == \"Fees\"\n",
        "\n",
        "      #Test 4\n",
        "      user_input = \"How long has the Alaskan Department of Snow been around?\"\n",
        "      result = classify_question(user_input)\n",
        "      assert result == \"General\"\n",
        "\n",
        "      print(\"PASSED: classify_question() tests\")\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"classify_question test FAILED: An error occurred - {e}\")\n",
        "\n",
        "def run_tests():\n",
        "\n",
        "  # Run tests for the snow removal tool & key agent functions (safe, valid, classification)\n",
        "  test_classify_questions()\n",
        "  test_is_safe()\n",
        "  test_is_valid()\n",
        "  test_calculate_snow_removal_fee()\n",
        "\n",
        "def run_eval_chat_with_tools_and_rag():\n",
        "\n",
        "  # Classification Questions Evaluation for groundedness and coherence\n",
        "  user_questions = [\n",
        "      \"What are the typical fees for snow removal in Alaska? Our property value is $370,000\",\n",
        "      \"How long has the Alaskan Department of Snow been around?\",\n",
        "      \"What is the fee?\",\n",
        "      \"Tell me about cats\",\n",
        "      \"How is the weather looking tomorrow?\",\n",
        "      \"How is the weather looking?\"\n",
        "  ]\n",
        "\n",
        "  responses =[]\n",
        "\n",
        "  for prompt in user_questions:\n",
        "    response = chat_with_tools_and_rag(prompt)\n",
        "    responses.append(response)\n",
        "    print(\"\\nUser Query: \" + prompt + \"\\nResponse: \" + response)\n",
        "\n",
        "  # Eval Dataset Setup\n",
        "  eval_dataset = pd.DataFrame({\n",
        "    \"prompt\": responses\n",
        "  })\n",
        "\n",
        "  eval_task = EvalTask(\n",
        "    dataset=eval_dataset,\n",
        "    metrics=[\n",
        "      MetricPromptTemplateExamples.Pointwise.FLUENCY,\n",
        "      MetricPromptTemplateExamples.Pointwise.COHERENCE\n",
        "    ],\n",
        "    experiment=\"main-genai-app-runtime\",\n",
        "  )\n",
        "\n",
        "  model = GenerativeModel(\n",
        "    \"gemini-2.0-flash-001\",\n",
        "    generation_config={\n",
        "        \"temperature\": 0,\n",
        "        \"top_p\": 0.4,\n",
        "    },\n",
        "  )\n",
        "\n",
        "  run_ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  eval_result = eval_task.evaluate(\n",
        "    model=model,\n",
        "    experiment_run_name=f\"apt-gen-{run_ts}\"\n",
        "  )\n",
        "\n",
        "  eval_result.metrics_table\n",
        "  print(eval_result.metrics_table)\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "\n",
        "  # Run Tests for key functions\n",
        "  run_tests()\n",
        "\n",
        "  # Run evaluation of the main GenAI runtime\n",
        "  run_eval_chat_with_tools_and_rag()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3Dpxq12gm_vZ",
        "outputId": "86e3c79f-341e-4d77-9c2a-307be7038cf5"
      },
      "id": "3Dpxq12gm_vZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSED: classify_question() tests\n",
            "PASSED: is_safe() tests\n",
            "PASSED: is_valid() tests\n",
            "PASSED: test_calculcated_snow_removal_fee() tests\n",
            "\n",
            "User Query: What are the typical fees for snow removal in Alaska? Our property value is $370,000\n",
            "Response: The annual snow removal fee for your property, valued at $370,000, is $370.\n",
            "\n",
            "\n",
            "User Query: How long has the Alaskan Department of Snow been around?\n",
            "Response: The Alaska Department of Snow (ADS) was established in 1959, coinciding with Alaska’s admission as a U.S. state.\n",
            "\n",
            "\n",
            "User Query: What is the fee?\n",
            "Response: I can calculate the annual snow removal fee for you. What is your property value?\n",
            "\n",
            "\n",
            "User Query: Tell me about cats\n",
            "Response: Your query does not seem to be related to the Alaska Snow Department. Please ask about snow removal in Alaska.\n",
            "\n",
            "User Query: How is the weather looking tomorrow?\n",
            "Response: I am sorry, I only have access to tonight's weather forecast. Tonight's forecast is: Isolated rain showers before 7pm. Mostly cloudy, with a low around 32. East wind around 10 mph. Chance of precipitation is 20%.\n",
            "\n",
            "\n",
            "User Query: How is the weather looking?\n",
            "Response: Tonight's forecast is mostly cloudy with a low around 32 degrees. There's a slight chance of isolated rain showers before 7pm, and the east wind will be around 10 mph. The chance of precipitation is 20%.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        \n",
              "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
              "    <style>\n",
              "      .view-vertex-resource,\n",
              "      .view-vertex-resource:hover,\n",
              "      .view-vertex-resource:visited {\n",
              "        position: relative;\n",
              "        display: inline-flex;\n",
              "        flex-direction: row;\n",
              "        height: 32px;\n",
              "        padding: 0 12px;\n",
              "          margin: 4px 18px;\n",
              "        gap: 4px;\n",
              "        border-radius: 4px;\n",
              "\n",
              "        align-items: center;\n",
              "        justify-content: center;\n",
              "        background-color: rgb(255, 255, 255);\n",
              "        color: rgb(51, 103, 214);\n",
              "\n",
              "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
              "        font-size: 13px;\n",
              "        font-weight: 500;\n",
              "        text-transform: uppercase;\n",
              "        text-decoration: none !important;\n",
              "\n",
              "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
              "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active {\n",
              "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
              "        position: absolute;\n",
              "        top: 0;\n",
              "        bottom: 0;\n",
              "        left: 0;\n",
              "        right: 0;\n",
              "        border-radius: 4px;\n",
              "        pointer-events: none;\n",
              "\n",
              "        content: '';\n",
              "        background-color: rgb(51, 103, 214);\n",
              "        opacity: 0.12;\n",
              "      }\n",
              "      .view-vertex-icon {\n",
              "        font-size: 18px;\n",
              "      }\n",
              "    </style>\n",
              "  \n",
              "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-bcdc2c55-71b2-416b-8bfc-b302989c7e66\" href=\"#view-view-vertex-resource-bcdc2c55-71b2-416b-8bfc-b302989c7e66\">\n",
              "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
              "          <span>View Experiment</span>\n",
              "        </a>\n",
              "        \n",
              "        <script>\n",
              "          (function () {\n",
              "            const link = document.getElementById('view-vertex-resource-bcdc2c55-71b2-416b-8bfc-b302989c7e66');\n",
              "            link.addEventListener('click', (e) => {\n",
              "              if (window.google?.colab?.openUrl) {\n",
              "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/main-genai-app-runtime/runs?project=qwiklabs-gcp-03-28c3125acb2b');\n",
              "              } else {\n",
              "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/main-genai-app-runtime/runs?project=qwiklabs-gcp-03-28c3125acb2b', '_blank');\n",
              "              }\n",
              "              e.stopPropagation();\n",
              "              e.preventDefault();\n",
              "            });\n",
              "          })();\n",
              "        </script>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.metadata.experiment_resources:Associating projects/416100490526/locations/us-central1/metadataStores/default/contexts/main-genai-app-runtime-apt-gen-20250429-205502 to Experiment: main-genai-app-runtime\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        \n",
              "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
              "    <style>\n",
              "      .view-vertex-resource,\n",
              "      .view-vertex-resource:hover,\n",
              "      .view-vertex-resource:visited {\n",
              "        position: relative;\n",
              "        display: inline-flex;\n",
              "        flex-direction: row;\n",
              "        height: 32px;\n",
              "        padding: 0 12px;\n",
              "          margin: 4px 18px;\n",
              "        gap: 4px;\n",
              "        border-radius: 4px;\n",
              "\n",
              "        align-items: center;\n",
              "        justify-content: center;\n",
              "        background-color: rgb(255, 255, 255);\n",
              "        color: rgb(51, 103, 214);\n",
              "\n",
              "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
              "        font-size: 13px;\n",
              "        font-weight: 500;\n",
              "        text-transform: uppercase;\n",
              "        text-decoration: none !important;\n",
              "\n",
              "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
              "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active {\n",
              "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
              "        position: absolute;\n",
              "        top: 0;\n",
              "        bottom: 0;\n",
              "        left: 0;\n",
              "        right: 0;\n",
              "        border-radius: 4px;\n",
              "        pointer-events: none;\n",
              "\n",
              "        content: '';\n",
              "        background-color: rgb(51, 103, 214);\n",
              "        opacity: 0.12;\n",
              "      }\n",
              "      .view-vertex-icon {\n",
              "        font-size: 18px;\n",
              "      }\n",
              "    </style>\n",
              "  \n",
              "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-df91b04d-3331-45c5-ae4c-94418422e946\" href=\"#view-view-vertex-resource-df91b04d-3331-45c5-ae4c-94418422e946\">\n",
              "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
              "          <span>View Experiment Run</span>\n",
              "        </a>\n",
              "        \n",
              "        <script>\n",
              "          (function () {\n",
              "            const link = document.getElementById('view-vertex-resource-df91b04d-3331-45c5-ae4c-94418422e946');\n",
              "            link.addEventListener('click', (e) => {\n",
              "              if (window.google?.colab?.openUrl) {\n",
              "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/main-genai-app-runtime/runs/main-genai-app-runtime-apt-gen-20250429-205502?project=qwiklabs-gcp-03-28c3125acb2b');\n",
              "              } else {\n",
              "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/main-genai-app-runtime/runs/main-genai-app-runtime-apt-gen-20250429-205502?project=qwiklabs-gcp-03-28c3125acb2b', '_blank');\n",
              "              }\n",
              "              e.stopPropagation();\n",
              "              e.preventDefault();\n",
              "            });\n",
              "          })();\n",
              "        </script>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vertexai.evaluation.eval_task:Logging Eval Experiment metadata: {'model_name': 'publishers/google/models/gemini-2.0-flash-001', 'temperature': 0, 'top_p': 0.4}\n",
            "INFO:vertexai.evaluation._evaluation:Generating a total of 6 responses from Gemini model gemini-2.0-flash-001.\n",
            "100%|██████████| 6/6 [00:01<00:00,  4.07it/s]\n",
            "INFO:vertexai.evaluation._evaluation:All 6 responses are successfully generated from Gemini model gemini-2.0-flash-001.\n",
            "INFO:vertexai.evaluation._evaluation:Multithreaded Batch Inference took: 1.489505163001013 seconds.\n",
            "INFO:vertexai.evaluation._evaluation:Computing metrics with a total of 12 Vertex Gen AI Evaluation Service API requests.\n",
            "100%|██████████| 12/12 [00:12<00:00,  1.01s/it]\n",
            "INFO:vertexai.evaluation._evaluation:All 12 metric requests are successfully computed.\n",
            "INFO:vertexai.evaluation._evaluation:Evaluation Took:12.113433942999109 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              prompt  \\\n",
            "0  The annual snow removal fee for your property,...   \n",
            "1  The Alaska Department of Snow (ADS) was establ...   \n",
            "2  I can calculate the annual snow removal fee fo...   \n",
            "3  Your query does not seem to be related to the ...   \n",
            "4  I am sorry, I only have access to tonight's we...   \n",
            "5  Tonight's forecast is mostly cloudy with a low...   \n",
            "\n",
            "                                            response  \\\n",
            "0  Okay, that means the snow removal fee is 0.1% ...   \n",
            "1  This is a clear and concise statement of fact....   \n",
            "2  I do not have a property value as I am an AI. ...   \n",
            "3  Okay, I understand.\\n\\n**What are the common m...   \n",
            "4  Okay, thanks for the information. So, to summa...   \n",
            "5  Okay, here's a summary of tonight's weather fo...   \n",
            "\n",
            "                                 fluency/explanation  fluency/score  \\\n",
            "0  The response is completely fluent because it d...            5.0   \n",
            "1  The response is fluent, employing sophisticate...            5.0   \n",
            "2  The response is completely fluent as it is fre...            5.0   \n",
            "3  The response is completely fluent as it is fre...            5.0   \n",
            "4  The response is completely fluent; it's well-o...            5.0   \n",
            "5  The response is completely fluent, free of gra...            5.0   \n",
            "\n",
            "                               coherence/explanation  coherence/score  \n",
            "0  The response has a clear and seamless logical ...              5.0  \n",
            "1  The response has a seamless and logical flow, ...              5.0  \n",
            "2  The response is completely coherent as it dire...              5.0  \n",
            "3  The response is completely coherent because it...              5.0  \n",
            "4  The response is completely coherent because it...              5.0  \n",
            "5  The response is completely coherent, presentin...              5.0  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "genai-challenge-05-jason-lajeunesse-without-widget"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}